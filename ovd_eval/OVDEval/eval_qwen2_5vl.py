import json
import argparse
import os
import torch
import numpy as np
import collections
from tqdm import tqdm
from PIL import Image
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
from transformers import AutoProcessor
from vllm import LLM, SamplingParams
from qwen_vl_utils import process_vision_info
import re
from typing import List, Dict, Any



def calculate_iou(box1, boxes2):
    """
    Calculate the intersection ratio (IoU)
    """
    x1, y1, w1, h1 = box1
    x2 = boxes2[:, 0]
    y2 = boxes2[:, 1]
    w2 = boxes2[:, 2]
    h2 = boxes2[:, 3]

    xmin = np.maximum(x1, x2)
    ymin = np.maximum(y1, y2)
    xmax = np.minimum(x1 + w1, x2 + w2)
    ymax = np.minimum(y1 + h1, y2 + h2)

    intersection = np.maximum(0, xmax - xmin) * np.maximum(0, ymax - ymin)
    union = w1 * h1 + w2 * h2 - intersection

    # Handle cases where the denominator is zero
    iou = np.where(union == 0, 0, intersection / union)

    return iou

def nms_threaded(gt, prediction, iou_threshold, num_threads):
    """NMSå¤„ç†"""
    if not prediction:
        return []
        
    gt_boxes = np.array([box['bbox'] for box in gt])
    boxes = np.array([box['bbox'] for box in prediction])
    scores = np.array([box['score'] for box in prediction])

    keep_list = []
    remove_list = []
    for idx, i in enumerate(gt):
        gt_box = gt_boxes[idx]
        iou = calculate_iou(gt_box, boxes)
        indices = np.where(iou > iou_threshold)[0].tolist()
        if indices:
            match_scores = scores[indices]
            sorted_indices = np.argsort(match_scores)[::-1]

            final_indices = []
            for sort_idx in sorted_indices.tolist():
                final_indices.append(indices[sort_idx])

            if final_indices:
                keep_list.append(final_indices[0])
                remove_list += final_indices[1:]

    final_remove_list = []
    for i in remove_list:
        if i not in keep_list:
            final_remove_list.append(i)

    selected_boxes = []
    for idx, i in enumerate(prediction):
        if idx not in final_remove_list:
            selected_boxes.append(i)

    return selected_boxes

def load_qwen2_5vl_model(model_path: str):
    """
    åŠ è½½ Qwen2.5-VL æ¨¡å‹ - ä½¿ç”¨ vLLM
    
    Args:
        model_path: æ¨¡å‹æƒé‡è·¯å¾„
    
    Returns:
        llm: vLLM æ¨¡å‹å®ä¾‹
        processor: æ•°æ®å¤„ç†å™¨
        sampling_params: é‡‡æ ·å‚æ•°
    """
    print(f"Loading Qwen2.5-VL model with vLLM from {model_path}...")
    
    # åŠ è½½æ¨¡å‹ - æŒ‰ç…§ä¿®æ”¹åçš„ qwen.py ä¸­çš„æ–¹å¼ï¼Œæ·»åŠ æ˜¾å­˜é™åˆ¶
    llm = LLM(
        model=model_path,
        limit_mm_per_prompt={"image": 10, "video": 10},
        gpu_memory_utilization=0.9,
        max_model_len=16384,  # é™åˆ¶æœ€å¤§åºåˆ—é•¿åº¦
    )
    
    # è®¾ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.0,  # è®¾ä¸º0ç¡®ä¿ç¡®å®šæ€§è¾“å‡º
        top_p=0.001,
        repetition_penalty=1.05,
        max_tokens=256,
        stop_token_ids=[],
    )
    
    # åŠ è½½å¤„ç†å™¨
    processor = AutoProcessor.from_pretrained(model_path)
    
    print("vLLM model loaded successfully!")
    
    return llm, processor, sampling_params

def parse_bbox_from_text(text: str, image_width: int, image_height: int):
    """
    ä»æ¨¡å‹è¾“å‡ºæ–‡æœ¬ä¸­è§£æè¾¹ç•Œæ¡†åæ ‡
    æ”¯æŒå¤šç§æ ¼å¼å’Œåæ ‡ç³»ç»Ÿ
    """
    bboxes = []
    
    # å®šä¹‰å¤šç§å¯èƒ½çš„è¾¹ç•Œæ¡†æ ¼å¼
    patterns = [
        # æ ‡å‡†çš„ <box> æ ¼å¼
        r'<box>(\d+),\s*(\d+),\s*(\d+),\s*(\d+)</box>',
        r'<\|box_start\|>(\d+),\s*(\d+),\s*(\d+),\s*(\d+)<\|box_end\|>',
        # æ‹¬å·æ ¼å¼
        r'\[(\d+),\s*(\d+),\s*(\d+),\s*(\d+)\]',
        r'\((\d+),\s*(\d+),\s*(\d+),\s*(\d+)\)',
        # åæ ‡å…³é”®è¯æ ¼å¼
        r'(?:coordinate|box|bbox|location).*?(\d+),\s*(\d+),\s*(\d+),\s*(\d+)',
        # ç®€å•çš„å››ä¸ªæ•°å­—æ ¼å¼ï¼ˆæœ€å®½æ¾ï¼‰
        r'(\d+),\s*(\d+),\s*(\d+),\s*(\d+)',
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches:
            print(f"    ğŸ¯ ä½¿ç”¨æ¨¡å¼ '{pattern}' æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…")
            break
    
    if not matches:
        print(f"    âš ï¸  åœ¨æ–‡æœ¬ä¸­æœªæ‰¾åˆ°è¾¹ç•Œæ¡†")
        print(f"    ğŸ“„ æ–‡æœ¬å†…å®¹: {repr(text[:200])}...")
        return bboxes
    
    for i, match in enumerate(matches):
        try:
            x1, y1, x2, y2 = map(int, match)
            # ç›´æ¥ä½¿ç”¨è§£æå‡ºçš„åæ ‡ï¼Œä¸è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
            print(f"    ğŸ“ åŸå§‹åæ ‡ {i+1}: ({x1}, {y1}, {x2}, {y2})")
            
            # æ£€æŸ¥åæ ‡æ˜¯å¦éœ€è¦äº¤æ¢ï¼ˆç¡®ä¿x1<x2, y1<y2ï¼‰
            if x1 > x2:
                x1, x2 = x2, x1
            if y1 > y2:
                y1, y2 = y2, y1
            
            # ç›´æ¥æŒ‰ç…§ GLIP çš„æ–¹å¼å¤„ç†ï¼šå‡è®¾è¾“å…¥æ˜¯ [x1, y1, x2, y2] æ ¼å¼
            w = x2 - x1
            h = y2 - y1
            bbox = [int(x1), int(y1), int(w), int(h)]
            print(f"    ğŸ“ è½¬æ¢ä¸º [x,y,w,h] æ ¼å¼: {bbox}")
            
            # éªŒè¯è¾¹ç•Œæ¡†çš„æœ‰æ•ˆæ€§
            if w > 0 and h > 0 and x1 >= 0 and y1 >= 0:
                bboxes.append(bbox)
                print(f"    âœ… æœ‰æ•ˆè¾¹ç•Œæ¡† {i+1}: {bbox}")
            else:
                print(f"    âŒ æ— æ•ˆè¾¹ç•Œæ¡† {i+1}: {bbox} (w={w}, h={h}, x1={x1}, y1={y1})")
                
        except ValueError as e:
            print(f"    âŒ åæ ‡è§£æé”™è¯¯: {match}, é”™è¯¯: {e}")
            continue
    
    print(f"    ğŸ“Š æœ€ç»ˆè§£æåˆ° {len(bboxes)} ä¸ªæœ‰æ•ˆè¾¹ç•Œæ¡†")
    return bboxes

def inference_qwen2_5vl(llm, processor, sampling_params, image_path: str, prompts: List[str], conf_threshold: float = 0.5):
    """
    ä½¿ç”¨ Qwen2.5-VL æ¨¡å‹è¿›è¡Œç›®æ ‡æ£€æµ‹æ¨ç† - vLLMç‰ˆæœ¬
    
    Args:
        llm: vLLM æ¨¡å‹å®ä¾‹
        processor: å¤„ç†å™¨
        sampling_params: é‡‡æ ·å‚æ•°
        image_path: å›¾ç‰‡è·¯å¾„
        prompts: æ£€æµ‹ç›®æ ‡åˆ—è¡¨
        conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
    
    Returns:
        results: æ£€æµ‹ç»“æœåˆ—è¡¨
    """
    image = Image.open(image_path).convert('RGB')
    image_width, image_height = image.size
    results = []
    
    # ä¸ºæ¯ä¸ªç›®æ ‡ç±»åˆ«å•ç‹¬è¿›è¡Œæ£€æµ‹
    for prompt in prompts:
        # å°è¯•å¤šç§ä¸åŒçš„æç¤ºè¯æ ¼å¼ï¼Œæ‰¾åˆ°æœ€æœ‰æ•ˆçš„
        detection_prompts = [
            f"Detect {prompt} in this image. Provide bounding boxes as <box>x1,y1,x2,y2</box>",
            f"Find all {prompt} objects and give their coordinates in <box>x1,y1,x2,y2</box> format",
            f"Locate {prompt} in the image and return bounding box coordinates",
            f"Can you find {prompt} in this image? Give me the bounding box",
            f"Where is the {prompt}? Provide coordinates"
        ]
        
        found_detection = False
        for detection_prompt in detection_prompts:
            if found_detection:
                break
                
            # å‡†å¤‡è¾“å…¥ - æŒ‰ç…§ä¿®æ”¹åçš„ qwen.py ä¸­çš„ vLLM æ–¹å¼
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "image": image_path,
                            "min_pixels": 224 * 224,
                            "max_pixels": 1280 * 28 * 28,
                        },
                        {"type": "text", "text": detection_prompt},
                    ],
                }
            ]
            
            # å¤„ç†è¾“å…¥ - æŒ‰ç…§ vLLM çš„æ–¹å¼ï¼Œä½¿ç”¨æœ€å…¼å®¹çš„æ–¹æ³•
            prompt_text = processor.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
            )
            
            # ä½¿ç”¨å…¼å®¹çš„æ–¹å¼å¤„ç†è§†è§‰ä¿¡æ¯
            try:
                # é¦–å…ˆå°è¯•æ–°ç‰ˆæœ¬ API
                image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)
            except TypeError:
                # å¦‚æœæ–°ç‰ˆæœ¬å¤±è´¥ï¼Œä½¿ç”¨æ—§ç‰ˆæœ¬ API
                try:
                    image_inputs, video_inputs = process_vision_info(messages)
                    video_kwargs = {}
                except Exception as e:
                    print(f"Warning: process_vision_info failed: {e}")
                    # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè·³è¿‡è¿™ä¸ªå›¾ç‰‡
                    continue
            
            mm_data = {}
            if image_inputs is not None:
                mm_data["image"] = image_inputs
            if video_inputs is not None:
                mm_data["video"] = video_inputs
            
            llm_inputs = {
                "prompt": prompt_text,
                "multi_modal_data": mm_data,
            }
            
            # åªæœ‰åœ¨æœ‰ video_kwargs æ—¶æ‰æ·»åŠ 
            if 'video_kwargs' in locals() and video_kwargs:
                llm_inputs["mm_processor_kwargs"] = video_kwargs
            
            # ç”Ÿæˆå›ç­” - ä½¿ç”¨ vLLM
            outputs = llm.generate([llm_inputs], sampling_params=sampling_params)
            output_text = outputs[0].outputs[0].text
            
            # æ‰“å°æ¨¡å‹è¾“å‡ºç”¨äºè°ƒè¯•
            print(f"\nğŸ” å›¾ç‰‡: {os.path.basename(image_path)}")
            print(f"ğŸ“ æ£€æµ‹ç›®æ ‡: {prompt}")
            print(f"ğŸ’¬ æç¤ºè¯: {detection_prompt}")
            print(f"ğŸ¤– æ¨¡å‹è¾“å‡º: {repr(output_text)}")
            print(f"ğŸ“ è¾“å‡ºé•¿åº¦: {len(output_text)} å­—ç¬¦")
            
            # è§£æè¾¹ç•Œæ¡†
            bboxes = parse_bbox_from_text(output_text, image_width, image_height)
            print(f"ğŸ“¦ è§£æç»“æœ: {len(bboxes)} ä¸ªè¾¹ç•Œæ¡†")
            
            # å¦‚æœæ‰¾åˆ°äº†è¾¹ç•Œæ¡†ï¼Œæ·»åŠ åˆ°ç»“æœä¸­å¹¶åœæ­¢å°è¯•å…¶ä»–æç¤ºè¯
            if bboxes:
                found_detection = True
                for bbox in bboxes:
                    results.append({
                        'bbox': bbox,
                        'score': conf_threshold,  # Qwen2.5-VL ä¸ç›´æ¥æä¾›ç½®ä¿¡åº¦ï¼Œä½¿ç”¨é˜ˆå€¼ä½œä¸ºé»˜è®¤å€¼
                        'label': prompt
                    })
    
    return results

def sample_images_by_category(gt_data, max_per_category=None, sample_ratio=None, max_total=None):
    """
    æ ¹æ®ç±»åˆ«é‡‡æ ·å›¾ç‰‡
    
    Args:
        gt_data: æ ‡æ³¨æ•°æ®
        max_per_category: æ¯ä¸ªç±»åˆ«æœ€å¤§å›¾ç‰‡æ•°
        sample_ratio: é‡‡æ ·æ¯”ä¾‹ (0.0-1.0)
        max_total: æ€»çš„æœ€å¤§å›¾ç‰‡æ•°
    
    Returns:
        sampled_images: é‡‡æ ·åçš„å›¾ç‰‡åˆ—è¡¨
    """
    import random
    from collections import defaultdict
    random.seed(7)
    if max_per_category is None and sample_ratio is None and max_total is None:
        return gt_data["images"]
    
    # æŒ‰ç±»åˆ«åˆ†ç»„å›¾ç‰‡
    category_images = defaultdict(list)
    
    # åˆ›å»ºå›¾ç‰‡IDåˆ°å›¾ç‰‡ä¿¡æ¯çš„æ˜ å°„
    image_id_to_image = {img["id"]: img for img in gt_data["images"]}
    
    # éå†æ ‡æ³¨ï¼Œä¸ºæ¯ä¸ªå›¾ç‰‡æ‰¾åˆ°å¯¹åº”çš„ç±»åˆ«
    for ann in gt_data["annotations"]:
        image_id = ann["image_id"]
        category_id = ann["category_id"]
        
        if image_id in image_id_to_image:
            img = image_id_to_image[image_id]
            category_images[category_id].append(img)
    
    # å»é‡ï¼ˆåŒä¸€å¼ å›¾ç‰‡å¯èƒ½æœ‰å¤šä¸ªæ ‡æ³¨ï¼‰
    for category_id in category_images:
        seen_ids = set()
        unique_images = []
        for img in category_images[category_id]:
            if img["id"] not in seen_ids:
                unique_images.append(img)
                seen_ids.add(img["id"])
        category_images[category_id] = unique_images
    
    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„å›¾ç‰‡æ•°é‡
    print(f"\nğŸ“Š æ•°æ®é‡‡æ ·ä¿¡æ¯:")
    category_names = {cat["id"]: cat["name"] for cat in gt_data["categories"]}
    
    sampled_images = []
    total_original = len(gt_data["images"])
    
    for category_id, images in category_images.items():
        category_name = category_names.get(category_id, f"Category_{category_id}")
        original_count = len(images)
        
        # ç¡®å®šé‡‡æ ·æ•°é‡
        if max_per_category:
            sample_count = min(original_count, max_per_category)
        elif sample_ratio:
            sample_count = max(1, int(original_count * sample_ratio))
        else:
            sample_count = original_count
        
        # éšæœºé‡‡æ ·
        if sample_count < original_count:
            sampled = random.sample(images, sample_count)
        else:
            sampled = images
            
        sampled_images.extend(sampled)
        print(f"  ğŸ“ {category_name}: {original_count} -> {sample_count} å¼ å›¾ç‰‡")
    
    # å»é‡ï¼ˆé˜²æ­¢åŒä¸€å¼ å›¾ç‰‡åœ¨å¤šä¸ªç±»åˆ«ä¸­ï¼‰
    seen_ids = set()
    unique_sampled = []
    for img in sampled_images:
        if img["id"] not in seen_ids:
            unique_sampled.append(img)
            seen_ids.add(img["id"])
    
    # å¦‚æœæŒ‡å®šäº†æ€»æ•°é™åˆ¶
    if max_total and len(unique_sampled) > max_total:
        unique_sampled = random.sample(unique_sampled, max_total)
        print(f"  ğŸ¯ æ€»æ•°é™åˆ¶: {len(sampled_images)} -> {max_total} å¼ å›¾ç‰‡")
    
    print(f"  ğŸ“ˆ æœ€ç»ˆé‡‡æ ·: {total_original} -> {len(unique_sampled)} å¼ å›¾ç‰‡")
    print(f"  ğŸ“Š é‡‡æ ·æ¯”ä¾‹: {len(unique_sampled)/total_original*100:.1f}%")
    
    return unique_sampled

def calculate_precision_recall(gt_boxes, pred_boxes, iou_threshold=0.7):
    """
    è®¡ç®—æŸ¥å‡†ç‡(Precision)å’ŒæŸ¥å…¨ç‡(Recall)
    
    Args:
        gt_boxes: çœŸå®æ ‡æ³¨æ¡†åˆ—è¡¨ [{'bbox': [x,y,w,h], 'category_id': int}, ...]
        pred_boxes: é¢„æµ‹æ¡†åˆ—è¡¨ [{'bbox': [x,y,w,h], 'category_id': int}, ...]
        iou_threshold: IoUé˜ˆå€¼
    
    Returns:
        precision: æŸ¥å‡†ç‡
        recall: æŸ¥å…¨ç‡
        tp: çœŸæ­£ä¾‹æ•°é‡
        fp: å‡æ­£ä¾‹æ•°é‡
        fn: å‡è´Ÿä¾‹æ•°é‡
    """
    if not pred_boxes and not gt_boxes:
        return 1.0, 1.0, 0, 0, 0  # éƒ½ä¸ºç©ºæ—¶è®¤ä¸ºå®Œå…¨æ­£ç¡®
    
    if not pred_boxes:
        return 0.0, 0.0, 0, 0, len(gt_boxes)  # æ²¡æœ‰é¢„æµ‹ï¼ŒæŸ¥å‡†ç‡å’ŒæŸ¥å…¨ç‡éƒ½æ˜¯0
    
    if not gt_boxes:
        return 0.0, 0.0, 0, len(pred_boxes), 0  # æ²¡æœ‰çœŸå®æ¡†ï¼ŒæŸ¥å‡†ç‡æ˜¯0
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„ä¾¿äºè®¡ç®—
    gt_array = np.array([box['bbox'] for box in gt_boxes])
    pred_array = np.array([box['bbox'] for box in pred_boxes])
    
    # è®¡ç®—æ‰€æœ‰é¢„æµ‹æ¡†ä¸çœŸå®æ¡†çš„IoU
    tp = 0  # çœŸæ­£ä¾‹
    fp = 0  # å‡æ­£ä¾‹
    matched_gt = set()  # å·²åŒ¹é…çš„çœŸå®æ¡†ç´¢å¼•
    
    for pred_idx, pred_box in enumerate(pred_boxes):
        pred_bbox = pred_box['bbox']
        best_iou = 0
        best_gt_idx = -1
        
        # æ‰¾åˆ°ä¸å½“å‰é¢„æµ‹æ¡†IoUæœ€å¤§çš„çœŸå®æ¡†
        for gt_idx, gt_box in enumerate(gt_boxes):
            if gt_idx in matched_gt:
                continue
                
            gt_bbox = gt_box['bbox']
            
            # æ£€æŸ¥ç±»åˆ«æ˜¯å¦åŒ¹é…ï¼ˆå¦‚æœæœ‰ç±»åˆ«ä¿¡æ¯ï¼‰
            if 'category_id' in pred_box and 'category_id' in gt_box:
                if pred_box['category_id'] != gt_box['category_id']:
                    continue
            
            # è®¡ç®—IoU
            iou = calculate_iou(pred_bbox, np.array([gt_bbox]))[0]
            
            if iou > best_iou:
                best_iou = iou
                best_gt_idx = gt_idx
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºçœŸæ­£ä¾‹
        if best_iou >= iou_threshold and best_gt_idx != -1:
            tp += 1
            matched_gt.add(best_gt_idx)
        else:
            fp += 1
    
    # è®¡ç®—å‡è´Ÿä¾‹ï¼ˆæœªè¢«åŒ¹é…çš„çœŸå®æ¡†ï¼‰
    fn = len(gt_boxes) - len(matched_gt)
    
    # è®¡ç®—æŸ¥å‡†ç‡å’ŒæŸ¥å…¨ç‡
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    
    return precision, recall, tp, fp, fn

def evaluate_dataset_precision_recall(gt_data, pred_results, iou_threshold=0.7):
    """
    è¯„ä¼°æ•´ä¸ªæ•°æ®é›†çš„æŸ¥å‡†ç‡å’ŒæŸ¥å…¨ç‡
    
    Args:
        gt_data: çœŸå®æ ‡æ³¨æ•°æ®
        pred_results: é¢„æµ‹ç»“æœåˆ—è¡¨
        iou_threshold: IoUé˜ˆå€¼
    
    Returns:
        dict: åŒ…å«æ€»ä½“å’Œå„ç±»åˆ«çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ç­‰æŒ‡æ ‡
    """
    # æŒ‰å›¾ç‰‡IDåˆ†ç»„
    gt_by_image = collections.defaultdict(list)
    pred_by_image = collections.defaultdict(list)
    
    # åˆ†ç»„çœŸå®æ ‡æ³¨
    for ann in gt_data['annotations']:
        gt_by_image[ann['image_id']].append({
            'bbox': ann['bbox'],
            'category_id': ann['category_id']
        })
    
    # åˆ†ç»„é¢„æµ‹ç»“æœ
    for pred in pred_results:
        pred_by_image[pred['image_id']].append({
            'bbox': pred['bbox'],
            'category_id': pred['category_id']
        })
    
    # ç»Ÿè®¡å„ç±»åˆ«çš„TP, FP, FN
    category_stats = collections.defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0})
    total_tp, total_fp, total_fn = 0, 0, 0
    
    # å¤„ç†æ¯å¼ å›¾ç‰‡
    all_image_ids = set(gt_by_image.keys()) | set(pred_by_image.keys())
    
    for image_id in all_image_ids:
        gt_boxes = gt_by_image.get(image_id, [])
        pred_boxes = pred_by_image.get(image_id, [])
        
        # è®¡ç®—è¯¥å›¾ç‰‡çš„precisionå’Œrecall
        precision, recall, tp, fp, fn = calculate_precision_recall(
            gt_boxes, pred_boxes, iou_threshold
        )
        
        total_tp += tp
        total_fp += fp
        total_fn += fn
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥æ›´ç»†åŒ–ï¼‰
        for gt_box in gt_boxes:
            cat_id = gt_box['category_id']
            # è¿™ä¸ªæ¡†æ˜¯å¦è¢«æ­£ç¡®æ£€æµ‹åˆ°ï¼Œç®€åŒ–åˆ¤æ–­
            category_stats[cat_id]['fn'] += 1  # å…ˆå‡è®¾éƒ½æ˜¯FNï¼Œåé¢ä¼šä¿®æ­£
            
        for pred_box in pred_boxes:
            cat_id = pred_box['category_id']
            category_stats[cat_id]['fp'] += 1  # å…ˆå‡è®¾éƒ½æ˜¯FPï¼Œåé¢ä¼šä¿®æ­£
    
    # è®¡ç®—æ€»ä½“æŒ‡æ ‡
    overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
    overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
    overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0.0
    
    # æ„å»ºç±»åˆ«åç§°æ˜ å°„
    category_names = {cat['id']: cat['name'] for cat in gt_data['categories']}
    
    results = {
        'iou_threshold': iou_threshold,
        'overall_metrics': {
            'precision': round(overall_precision, 4),
            'recall': round(overall_recall, 4),
            'f1_score': round(overall_f1, 4),
            'tp': total_tp,
            'fp': total_fp,
            'fn': total_fn,
            'total_gt': total_tp + total_fn,
            'total_pred': total_tp + total_fp
        },
        'detailed_stats': {
            'total_images_processed': len(all_image_ids),
            'images_with_gt': len(gt_by_image),
            'images_with_predictions': len(pred_by_image)
        }
    }
    
    return results

def main():
    parser = argparse.ArgumentParser(
        description="Evaluate Qwen2.5-VL model on OVDEval",
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æ¯ä¸ªç±»åˆ«æœ€å¤šå¤„ç†10å¼ å›¾ç‰‡
  python eval_qwen2_5vl.py --gt-path data.json --image-path imgs/ --model-path model/ --max-per-category 10
  
  # éšæœºé‡‡æ ·20%çš„æ•°æ®
  python eval_qwen2_5vl.py --gt-path data.json --image-path imgs/ --model-path model/ --sample-ratio 0.2
  
  # æ€»å…±æœ€å¤šå¤„ç†50å¼ å›¾ç‰‡
  python eval_qwen2_5vl.py --gt-path data.json --image-path imgs/ --model-path model/ --max-images 50
  
  # ç»„åˆä½¿ç”¨ï¼šæ¯ç±»æœ€å¤š5å¼ ï¼Œæ€»å…±ä¸è¶…è¿‡30å¼ 
  python eval_qwen2_5vl.py --gt-path data.json --image-path imgs/ --model-path model/ --max-per-category 5 --max-images 30
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument("--gt-path", type=str, required=True, help="Ground truth annotation file path")
    parser.add_argument("--image-path", type=str, required=True, help="Image directory path")
    parser.add_argument("--model-path", type=str, required=True, help="Qwen2.5-VL model path")
    parser.add_argument("--output-path", type=str, default="/home/chuziyuan/zju/qwen_tamp/ovd_eval/OVDEval/output", help="Output directory path")
    parser.add_argument("--iou-threshold", type=float, default=0.7, help="IoU threshold for NMS")
    parser.add_argument("--num-threads", type=int, default=16, help="Number of threads for NMS")
    parser.add_argument("--conf-threshold", type=float, default=0.5, help="Confidence threshold")
    parser.add_argument("--max-images", type=int, default=None, help="æ€»çš„æœ€å¤§å›¾ç‰‡æ•°é‡é™åˆ¶")
    parser.add_argument("--max-per-category", type=int, default=None, help="æ¯ä¸ªç±»åˆ«æœ€å¤§å›¾ç‰‡æ•° (ä¾‹å¦‚: 10)")
    parser.add_argument("--sample-ratio", type=float, default=None, help="é‡‡æ ·æ¯”ä¾‹ 0.0-1.0 (ä¾‹å¦‚: 0.2è¡¨ç¤º20%)")

    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if not os.path.exists(args.output_path):
        os.makedirs(args.output_path, exist_ok=True)
    
    # åŠ è½½æ¨¡å‹
    llm, processor, sampling_params = load_qwen2_5vl_model(args.model_path)
    print("Qwen2.5-VL model with vLLM loaded successfully!")
    
    # åŠ è½½æ ‡æ³¨æ•°æ®
    gt_data = json.load(open(args.gt_path))
    dataset_name = args.gt_path.rsplit('/', 1)[-1].split('.')[0]
    print(f"==============Now testing {dataset_name}.")
    
    before_nms_results = []
    category_map = {}

    for cat in gt_data["categories"]:
        category_map[cat["name"]] = cat["id"]
    
    # å¤„ç†å›¾ç‰‡ - æ™ºèƒ½é‡‡æ ·
    print(f"åŸå§‹æ•°æ®é›†åŒ…å« {len(gt_data['images'])} å¼ å›¾ç‰‡")
    
    # æ ¹æ®å‚æ•°è¿›è¡Œé‡‡æ ·
    images_to_process = sample_images_by_category(
        gt_data, 
        max_per_category=args.max_per_category,
        sample_ratio=args.sample_ratio,
        max_total=args.max_images
    )
    
    if len(images_to_process) != len(gt_data["images"]):
        print(f"âœ‚ï¸  æ•°æ®é‡‡æ ·å®Œæˆ: {len(gt_data['images'])} -> {len(images_to_process)} å¼ å›¾ç‰‡")
    
    # å¯¹æ¯å¼ å›¾ç‰‡è¿›è¡Œæ¨ç†
    total_images = len(images_to_process)
    for idx, img in enumerate(tqdm(images_to_process, desc="Processing images")):
        prompt = []
        img_path = os.path.join(args.image_path, img["file_name"])
        
        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
        if not os.path.exists(img_path):
            print(f"Image not found: {img_path}")
            continue
        
        # æ ¹æ®æ•°æ®é›†ç±»å‹ç¡®å®šæç¤ºè¯
        if dataset_name in ['coco', 'celebrity', 'logo', 'landmark']:
            for cat in gt_data["categories"]:
                prompt.append(cat["name"])
        else:
            for t in img["text"] + img["neg_text"]:
                prompt.append(t)

        # è¿›è¡Œæ¨ç†
        print(f"\n{'='*50}")
        print(f"ğŸ“¸ å¤„ç†å›¾ç‰‡ {idx+1}/{total_images}: {img['file_name']}")
        print(f"ğŸ¯ æ£€æµ‹ç›®æ ‡: {prompt}")
        print(f"{'='*50}")
        
        try:
            resp = inference_qwen2_5vl(
                llm, processor, sampling_params, img_path, prompt, args.conf_threshold
            )
            
            # å¤„ç†æ¨ç†ç»“æœ
            for pred in resp:
                pred_image = {"image_id": img["id"]}
                bbox = pred['bbox']
                
                label = category_map.get(pred['label'], 0)
                score = pred['score']
                
                pred_image["score"] = round(float(score), 8)
                pred_image["category_id"] = label
                pred_image["bbox"] = bbox
                
                before_nms_results.append(pred_image)
            
            print(f"âœ… å›¾ç‰‡å¤„ç†å®Œæˆï¼Œæ£€æµ‹åˆ° {len(resp)} ä¸ªç›®æ ‡")
                
        except Exception as e:
            print(f"âŒ å›¾ç‰‡å¤„ç†å¤±è´¥: {img_path}")
            print(f"é”™è¯¯ä¿¡æ¯: {e}")
            continue

    # NMS å¤„ç†
    print("==============Before NMS:", len(before_nms_results))

    gt_data_ann = gt_data['annotations']
    
    image_dict = collections.defaultdict(list)
    gt_dict = collections.defaultdict(list)

    for i in before_nms_results:
        image_dict[i["image_id"]].append(i)

    for i in gt_data_ann:
        gt_dict[i["image_id"]].append(i)
    
    after_nms_results = []
    # è°ƒç”¨å¤šçº¿ç¨‹å¹¶è¡ŒåŒ– NMS å‡½æ•°
    for img, preds in image_dict.items():
        gts = gt_dict.get(img, [])
        if gts:  # åªæœ‰å½“å­˜åœ¨gtæ—¶æ‰è¿›è¡ŒNMS
            selected_boxes = nms_threaded(gts, preds, args.iou_threshold, args.num_threads)
            after_nms_results += selected_boxes
        else:
            after_nms_results += preds
    
    print("==============After NMS:", len(after_nms_results))
    
    # è®¡ç®—æŸ¥å‡†ç‡å’ŒæŸ¥å…¨ç‡
    if after_nms_results:
        print(f"\n==============è®¡ç®— {dataset_name} æ•°æ®é›†çš„æŸ¥å‡†ç‡å’ŒæŸ¥å…¨ç‡")
        
        # è¯„ä¼°å½“å‰æ•°æ®é›†
        evaluation_results = evaluate_dataset_precision_recall(
            gt_data, after_nms_results, args.iou_threshold
        )
        
        # ä¿å­˜è¯¥æ•°æ®é›†çš„è¯„ä¼°ç»“æœ
        eval_save_path = os.path.join(args.output_path, f"{dataset_name}_precision_recall.json")
        json.dump(evaluation_results, open(eval_save_path, 'w'), indent=2, ensure_ascii=False)
        
        # æ‰“å°ç»“æœ
        overall = evaluation_results['overall_metrics']
        print(f"ğŸ“Š {dataset_name} æ•°æ®é›†è¯„ä¼°ç»“æœ:")
        print(f"   ğŸ¯ æŸ¥å‡†ç‡ (Precision): {overall['precision']:.4f}")
        print(f"   ğŸ“ˆ æŸ¥å…¨ç‡ (Recall): {overall['recall']:.4f}")
        print(f"   ğŸ† F1 åˆ†æ•°: {overall['f1_score']:.4f}")
        print(f"   âœ… çœŸæ­£ä¾‹ (TP): {overall['tp']}")
        print(f"   âŒ å‡æ­£ä¾‹ (FP): {overall['fp']}")
        print(f"   âš ï¸  å‡è´Ÿä¾‹ (FN): {overall['fn']}")
        print(f"   ğŸ“ IoU é˜ˆå€¼: {evaluation_results['iou_threshold']}")
        
        # ä¿å­˜åŸå§‹æ£€æµ‹ç»“æœï¼ˆå¯é€‰ï¼‰
        det_save_path = os.path.join(args.output_path, f"{dataset_name}_detections.json")
        json.dump(after_nms_results, open(det_save_path, 'w'), indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {eval_save_path}")
        print(f"ğŸ’¾ æ£€æµ‹ç»“æœå·²ä¿å­˜åˆ°: {det_save_path}")
        
        return evaluation_results  # è¿”å›ç»“æœç”¨äºåç»­æ•´ä½“å¹³å‡è®¡ç®—
    else:
        print("âŒ æ²¡æœ‰æ£€æµ‹ç»“æœç”¨äºè¯„ä¼°!")
        return None

if __name__ == "__main__":
    main() 
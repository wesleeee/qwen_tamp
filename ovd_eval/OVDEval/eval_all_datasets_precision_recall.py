#!/usr/bin/env python3
"""
æ‰¹é‡è¯„ä¼°æ‰€æœ‰OVDEvalæ•°æ®é›†çš„æŸ¥å‡†ç‡å’ŒæŸ¥å…¨ç‡
è®¡ç®—æ¯ä¸ªæ•°æ®é›†çš„æŒ‡æ ‡å¹¶ä¿å­˜æ•´ä½“å¹³å‡ç»“æœ
"""

import argparse
import json
import os
import sys
from typing import List, Dict, Any
import subprocess

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥eval_qwen2_5vlæ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def run_single_dataset_evaluation(dataset_name: str, args) -> Dict[str, Any]:
    """
    è¿è¡Œå•ä¸ªæ•°æ®é›†çš„è¯„ä¼°
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        args: å‘½ä»¤è¡Œå‚æ•°
        
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    # æ„å»ºæ•°æ®è·¯å¾„
    gt_path = os.path.join(args.data_root, f"{dataset_name}.json")
    image_path = os.path.join(args.data_root, f"{dataset_name}/")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(gt_path):
        print(f"âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶ {gt_path}")
        return None
        
    if not os.path.exists(image_path):
        print(f"âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ°å›¾ç‰‡ç›®å½• {image_path}")
        return None
    
    print(f"\n{'='*60}")
    print(f"ğŸ” å¼€å§‹è¯„ä¼°æ•°æ®é›†: {dataset_name}")
    print(f"{'='*60}")
    
    # æ„å»ºå‘½ä»¤
    cmd = [
        "python", "eval_qwen2_5vl.py",
        "--gt-path", gt_path,
        "--image-path", image_path,
        "--model-path", args.model_path,
        "--output-path", args.output_path,
        "--iou-threshold", str(args.iou_threshold),
        "--conf-threshold", str(args.conf_threshold)
    ]
    
    # æ·»åŠ å¯é€‰å‚æ•°
    if args.max_images:
        cmd.extend(["--max-images", str(args.max_images)])
    if args.max_per_category:
        cmd.extend(["--max-per-category", str(args.max_per_category)])
    if args.sample_ratio:
        cmd.extend(["--sample-ratio", str(args.sample_ratio)])
    
    try:
        # è¿è¡Œè¯„ä¼°è„šæœ¬
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.path.dirname(__file__))
        
        if result.returncode != 0:
            print(f"âŒ æ•°æ®é›† {dataset_name} è¯„ä¼°å¤±è´¥:")
            print(f"é”™è¯¯è¾“å‡º: {result.stderr}")
            return None
        
        # è¯»å–è¯„ä¼°ç»“æœ
        eval_result_path = os.path.join(args.output_path, f"{dataset_name}_precision_recall.json")
        
        if os.path.exists(eval_result_path):
            with open(eval_result_path, 'r') as f:
                evaluation_results = json.load(f)
            
            print(f"âœ… æ•°æ®é›† {dataset_name} è¯„ä¼°å®Œæˆ")
            overall = evaluation_results['overall_metrics']
            print(f"   ğŸ¯ æŸ¥å‡†ç‡: {overall['precision']:.4f}")
            print(f"   ğŸ“ˆ æŸ¥å…¨ç‡: {overall['recall']:.4f}")
            print(f"   ğŸ† F1åˆ†æ•°: {overall['f1_score']:.4f}")
            
            return evaluation_results
        else:
            print(f"âŒ æ‰¾ä¸åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶: {eval_result_path}")
            return None
            
    except Exception as e:
        print(f"âŒ è¯„ä¼°æ•°æ®é›† {dataset_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

def calculate_average_metrics(all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    è®¡ç®—æ‰€æœ‰æ•°æ®é›†çš„å¹³å‡æŒ‡æ ‡
    
    Args:
        all_results: æ‰€æœ‰æ•°æ®é›†çš„è¯„ä¼°ç»“æœåˆ—è¡¨
        
    Returns:
        å¹³å‡æŒ‡æ ‡å­—å…¸
    """
    if not all_results:
        return {}
    
    # ç»Ÿè®¡æ€»ä½“æŒ‡æ ‡
    total_tp = sum(result['overall_metrics']['tp'] for result in all_results)
    total_fp = sum(result['overall_metrics']['fp'] for result in all_results)
    total_fn = sum(result['overall_metrics']['fn'] for result in all_results)
    
    # è®¡ç®—å®å¹³å‡ï¼ˆæ¯ä¸ªæ•°æ®é›†æƒé‡ç›¸ç­‰ï¼‰
    macro_precision = sum(result['overall_metrics']['precision'] for result in all_results) / len(all_results)
    macro_recall = sum(result['overall_metrics']['recall'] for result in all_results) / len(all_results)
    macro_f1 = sum(result['overall_metrics']['f1_score'] for result in all_results) / len(all_results)
    
    # è®¡ç®—å¾®å¹³å‡ï¼ˆæ¯ä¸ªæ ·æœ¬æƒé‡ç›¸ç­‰ï¼‰
    micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
    micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
    micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0.0
    
    # ç»Ÿè®¡è¯¦ç»†ä¿¡æ¯
    total_images = sum(result['detailed_stats']['total_images_processed'] for result in all_results)
    total_gt_images = sum(result['detailed_stats']['images_with_gt'] for result in all_results)
    total_pred_images = sum(result['detailed_stats']['images_with_predictions'] for result in all_results)
    
    return {
        'evaluation_summary': {
            'total_datasets': len(all_results),
            'iou_threshold': all_results[0]['iou_threshold'],
            'evaluation_date': None  # å¯ä»¥æ·»åŠ æ—¶é—´æˆ³
        },
        'macro_average': {
            'precision': round(macro_precision, 4),
            'recall': round(macro_recall, 4),
            'f1_score': round(macro_f1, 4)
        },
        'micro_average': {
            'precision': round(micro_precision, 4),
            'recall': round(micro_recall, 4),
            'f1_score': round(micro_f1, 4),
            'total_tp': total_tp,
            'total_fp': total_fp,
            'total_fn': total_fn,
            'total_gt': total_tp + total_fn,
            'total_pred': total_tp + total_fp
        },
        'dataset_details': {
            'total_images_processed': total_images,
            'total_images_with_gt': total_gt_images,
            'total_images_with_predictions': total_pred_images
        },
        'individual_results': {
            result.get('dataset_name', f'dataset_{i}'): result['overall_metrics'] 
            for i, result in enumerate(all_results)
        }
    }

def main():
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡è¯„ä¼°æ‰€æœ‰OVDEvalæ•°æ®é›†çš„æŸ¥å‡†ç‡å’ŒæŸ¥å…¨ç‡",
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # è¯„ä¼°æ‰€æœ‰æ•°æ®é›†
  python eval_all_datasets_precision_recall.py --data-root /path/to/OVDEval --model-path /path/to/model --output-path ./output
  
  # å¿«é€Ÿæµ‹è¯•ï¼ˆæ¯ä¸ªæ•°æ®é›†åªå¤„ç†å°‘é‡å›¾ç‰‡ï¼‰
  python eval_all_datasets_precision_recall.py --data-root /path/to/OVDEval --model-path /path/to/model --max-images 10
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument("--data-root", type=str, required=True, help="OVDEvalæ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--model-path", type=str, required=True, help="Qwen2.5-VLæ¨¡å‹è·¯å¾„")
    parser.add_argument("--output-path", type=str, default="./output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--iou-threshold", type=float, default=0.7, help="IoUé˜ˆå€¼")
    parser.add_argument("--conf-threshold", type=float, default=0.9, help="ç½®ä¿¡åº¦é˜ˆå€¼")
    parser.add_argument("--max-images", type=int, default=None, help="æ¯ä¸ªæ•°æ®é›†æœ€å¤§å›¾ç‰‡æ•°")
    parser.add_argument("--max-per-category", type=int, default=None, help="æ¯ä¸ªç±»åˆ«æœ€å¤§å›¾ç‰‡æ•°")
    parser.add_argument("--sample-ratio", type=float, default=None, help="é‡‡æ ·æ¯”ä¾‹")
    parser.add_argument("--datasets", nargs='+', default=None, 
                       help="æŒ‡å®šè¦è¯„ä¼°çš„æ•°æ®é›†åç§°ï¼Œé»˜è®¤è¯„ä¼°æ‰€æœ‰æ•°æ®é›†")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_path, exist_ok=True)
    
    # é»˜è®¤æ•°æ®é›†åˆ—è¡¨
    default_datasets = ["material", "color", "position", "relationship", "negation", "celebrity", "logo", "landmark"]
    
    # ç¡®å®šè¦è¯„ä¼°çš„æ•°æ®é›†
    datasets_to_evaluate = args.datasets if args.datasets else default_datasets
    
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡è¯„ä¼° {len(datasets_to_evaluate)} ä¸ªæ•°æ®é›†")
    print(f"ğŸ“ æ•°æ®æ ¹ç›®å½•: {args.data_root}")
    print(f"ğŸ¤– æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {args.output_path}")
    print(f"ğŸ¯ IoUé˜ˆå€¼: {args.iou_threshold}")
    
    if args.max_images:
        print(f"ğŸ”¢ æ¯ä¸ªæ•°æ®é›†æœ€å¤§å›¾ç‰‡æ•°: {args.max_images}")
    
    # è¯„ä¼°æ‰€æœ‰æ•°æ®é›†
    all_results = []
    successful_datasets = []
    failed_datasets = []
    
    for dataset_name in datasets_to_evaluate:
        result = run_single_dataset_evaluation(dataset_name, args)
        if result:
            result['dataset_name'] = dataset_name  # æ·»åŠ æ•°æ®é›†åç§°
            all_results.append(result)
            successful_datasets.append(dataset_name)
        else:
            failed_datasets.append(dataset_name)
    
    # è®¡ç®—æ•´ä½“å¹³å‡æŒ‡æ ‡
    if all_results:
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è®¡ç®—æ•´ä½“å¹³å‡æŒ‡æ ‡")
        print(f"{'='*60}")
        
        average_results = calculate_average_metrics(all_results)
        
        # ä¿å­˜æ•´ä½“ç»“æœ
        overall_save_path = os.path.join(args.output_path, "overall_precision_recall_summary.json")
        with open(overall_save_path, 'w') as f:
            json.dump(average_results, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°æ•´ä½“ç»“æœ
        macro_avg = average_results['macro_average']
        micro_avg = average_results['micro_average']
        
        print(f"ğŸ“ˆ å®å¹³å‡ç»“æœ (Macro Average):")
        print(f"   ğŸ¯ æŸ¥å‡†ç‡: {macro_avg['precision']:.4f}")
        print(f"   ğŸ“ˆ æŸ¥å…¨ç‡: {macro_avg['recall']:.4f}")
        print(f"   ğŸ† F1åˆ†æ•°: {macro_avg['f1_score']:.4f}")
        
        print(f"\nğŸ“Š å¾®å¹³å‡ç»“æœ (Micro Average):")
        print(f"   ğŸ¯ æŸ¥å‡†ç‡: {micro_avg['precision']:.4f}")
        print(f"   ğŸ“ˆ æŸ¥å…¨ç‡: {micro_avg['recall']:.4f}")
        print(f"   ğŸ† F1åˆ†æ•°: {micro_avg['f1_score']:.4f}")
        print(f"   âœ… æ€»çœŸæ­£ä¾‹: {micro_avg['total_tp']}")
        print(f"   âŒ æ€»å‡æ­£ä¾‹: {micro_avg['total_fp']}")
        print(f"   âš ï¸  æ€»å‡è´Ÿä¾‹: {micro_avg['total_fn']}")
        
        print(f"\nğŸ’¾ æ•´ä½“ç»“æœå·²ä¿å­˜åˆ°: {overall_save_path}")
        
        # æˆåŠŸ/å¤±è´¥ç»Ÿè®¡
        print(f"\nğŸ“Š è¯„ä¼°ç»Ÿè®¡:")
        print(f"   âœ… æˆåŠŸè¯„ä¼°: {len(successful_datasets)} ä¸ªæ•°æ®é›†")
        if successful_datasets:
            print(f"      {', '.join(successful_datasets)}")
        
        if failed_datasets:
            print(f"   âŒ å¤±è´¥æ•°æ®é›†: {len(failed_datasets)} ä¸ª")
            print(f"      {', '.join(failed_datasets)}")
    
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸè¯„ä¼°ä»»ä½•æ•°æ®é›†!")
        sys.exit(1)

if __name__ == "__main__":
    main() 